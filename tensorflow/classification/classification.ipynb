{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yVZy1PYIfrG"
      },
      "source": [
        "## Get the TensorRT tar file before running this Notebook\n",
        "\n",
        "1. Visit https://developer.nvidia.com/tensorrt\n",
        "2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n",
        "3. Now, in the download page: Choose TensorRT 8 in available versions\n",
        "4. Agree to Terms and Conditions\n",
        "5. Click on TensorRT 8.6 GA to expand the available options\n",
        "6. Click on 'TensorRT 8.6 GA for Linux x86_64 and CUDA 12.0 and 12.1 TAR Package' to dowload the TAR file\n",
        "7. Upload the the tar file to your Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLBrzF8hKgS"
      },
      "source": [
        "## Connect to GPU Instance\n",
        "\n",
        "1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n",
        "1. Then click on Connect (Top Right)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpjyNg5c2V9"
      },
      "source": [
        "## Mounting Google drive\n",
        "Mount your Google drive storage to this Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUVkYw0hzqG",
        "outputId": "b5a05714-5742-418e-e1c9-b8692c36ff5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_COLAB=1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    %env GOOGLE_COLAB=1\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    %env GOOGLE_COLAB=0\n",
        "    print(\"Warning: Not a Colab Environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKV53RnEhxu7"
      },
      "source": [
        "# TAO Image Classification\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n",
        "\n",
        "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sgNEt9Mhxu-"
      },
      "source": [
        "## Learning Objectives\n",
        "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
        "\n",
        "* Take a pretrained resnet18 model and finetune on a sample dataset converted from PascalVOC\n",
        "* Prune the finetuned model\n",
        "* Retrain the pruned model to recover lost accuracy\n",
        "* Export the pruned model\n",
        "* Run Inference on the trained model\n",
        "* Export the pruned and retrained model to a .etlt file for deployment to DeepStream\n",
        "\n",
        "### Table of Contents\n",
        "This notebook shows an example use case for classification using the Train Adapt Optimize (TAO) Toolkit.\n",
        "\n",
        "0. [Set up env variables](#head-0)\n",
        "1. [Prepare dataset and pretrained model](#head-1)\n",
        "    1. [Split the dataset into train/test/val](#head-1-1)\n",
        "    2. [Download pre-trained model](#head-1-2)\n",
        "2. [Setup GPU environment](#head-2) <br>\n",
        "    2.1 [Setup Python environment](#head-2-1) <br>\n",
        "3. [Provide training specification](#head-3)\n",
        "4. [Run TAO training](#head-4)\n",
        "5. [Evaluate trained models](#head-5)\n",
        "6. [Prune trained models](#head-6)\n",
        "7. [Retrain pruned models](#head-7)\n",
        "8. [Testing the model](#head-8)\n",
        "9. [Visualize inferences](#head-9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBi_JDq3hxu-"
      },
      "source": [
        "#### Note\n",
        "1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n",
        "2. This notebook uses VOC dataset by default, which should be around ~3 GB.\n",
        "3. Using the default config/spec file provided in this notebook, each weight file size of classification created during training will be ~88 MB\n",
        "\n",
        "## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n",
        "\n",
        "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n",
        "\n",
        "#### FIXME\n",
        "1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n",
        "1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n",
        "1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n",
        "1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n",
        "1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n",
        "1. delete_existing_data - set this to True to remove existing preprocessed and original data\n",
        "1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n",
        "1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n",
        "1. trt_version - set this to the version of TRT you have downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mguwyH6dhxvC"
      },
      "source": [
        "## 1. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw5T4INKhxu_",
        "outputId": "fbab049a-d04e-4660-f1f1-5adb2315d71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TAO_DOCKER_DISABLE=1\n",
            "env: KEY=nvidia_tlt\n",
            "env: NUM_GPUS=1\n",
            "env: COLAB_NOTEBOOKS_PATH=/localhome/local-rarunachalam/colab_notebooks\n",
            "Cloning into '/localhome/local-rarunachalam/colab_notebooks'...\n",
            "remote: Enumerating objects: 2657, done.\u001b[K\n",
            "remote: Counting objects: 100% (350/350), done.\u001b[K\n",
            "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
            "remote: Total 2657 (delta 241), reused 251 (delta 157), pack-reused 2307 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2657/2657), 4.05 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (1735/1735), done.\n",
            "env: EXPERIMENT_DIR=/content/drive/MyDrive/TAO_Hiwi/results/classification\n",
            "env: DATA_DIR=/content/drive/MyDrive/TAO_Hiwi/data_images\n",
            "env: SPECS_DIR=/localhome/local-rarunachalam/colab_notebooks/tensorflow/classification/specs\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1175 Nov 30 15:58 classification_spec.cfg\n",
            "-rw-r--r-- 1 root root 1046 Nov 30 15:58 classification_retrain_spec.cfg\n"
          ]
        }
      ],
      "source": [
        "# Setting up env variables for cleaner command line commands.\n",
        "import os\n",
        "\n",
        "%env TAO_DOCKER_DISABLE=1\n",
        "\n",
        "%env KEY=nvidia_tlt\n",
        "#FIXME1\n",
        "%env NUM_GPUS=1\n",
        "\n",
        "#FIXME2\n",
        "%env COLAB_NOTEBOOKS_PATH=/localhome/local-rarunachalam/colab_notebooks\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n",
        "\n",
        "      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n",
        "else:\n",
        "    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
        "        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n",
        "\n",
        "#FIXME3\n",
        "%env EXPERIMENT_DIR=/content/drive/MyDrive/TAO_Hiwi/results/classification\n",
        "#FIXME4\n",
        "delete_existing_experiments = True\n",
        "#FIXME5\n",
        "%env DATA_DIR=/content/drive/MyDrive/TAO_Hiwi/data_images\n",
        "#FIXME6\n",
        "delete_existing_data = False\n",
        "\n",
        "if delete_existing_experiments:\n",
        "    !sudo rm -rf $EXPERIMENT_DIR\n",
        "if delete_existing_data:\n",
        "    !sudo rm -rf $DATA_DIR\n",
        "\n",
        "SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/classification/specs\"\n",
        "%env SPECS_DIR={SPECS_DIR}\n",
        "# Showing list of specification files.\n",
        "!ls -rlt $SPECS_DIR\n",
        "\n",
        "!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n",
        "!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAvz0tgVhxvC"
      },
      "source": [
        "We will be using the pascal VOC dataset for the tutorial. To find more details please visit\n",
        "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit. Please download the dataset present at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to $DATA_DIR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YvLSpMachxvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62e49f6-c7db-4872-874c-a072d341596f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dataset.\n"
          ]
        }
      ],
      "source": [
        "# Check that file is present\n",
        "import os\n",
        "DATA_DIR = os.environ.get('DATA_DIR')\n",
        "if not os.path.isfile(os.path.join(DATA_DIR , 'VOCtrainval_11-May-2012.tar')):\n",
        "    print('tar file for dataset not found. Please download.')\n",
        "else:\n",
        "    print('Found dataset.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udd0_37KhxvC"
      },
      "outputs": [],
      "source": [
        "# unpack\n",
        "!tar -xvf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ojwdhEEmhxvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82825ebe-48cc-45d9-f3c3-17d8df43182f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations  ImageSets\tJPEGImages  SegmentationClass  SegmentationObject\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "!ls $DATA_DIR/VOCdevkit/VOC2012"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8ZaG48hxvD"
      },
      "source": [
        "### A. Split the dataset into train/val/test <a class=\"anchor\" id=\"head-2-1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4O1bDOQhxvD"
      },
      "source": [
        "Pascal VOC Dataset is converted to our format (for classification) and then to train/val/test in the next two blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sRYTZ-xhxvE"
      },
      "outputs": [],
      "source": [
        "from os.path import join as join_path\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "DATA_DIR=os.environ.get('DATA_DIR')\n",
        "source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n",
        "target_dir = join_path(DATA_DIR, \"formatted\")\n",
        "\n",
        "\n",
        "suffix = '_trainval.txt'\n",
        "classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n",
        "images_dir = join_path(source_dir, \"JPEGImages\")\n",
        "classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n",
        "for file in classes_files:\n",
        "    # get the filename and make output class folder\n",
        "    classname = os.path.basename(file)\n",
        "    if classname.endswith(suffix):\n",
        "        classname = classname[:-len(suffix)]\n",
        "        target_dir_path = join_path(target_dir, classname)\n",
        "        if not os.path.exists(target_dir_path):\n",
        "            os.makedirs(target_dir_path)\n",
        "    else:\n",
        "        continue\n",
        "    print(classname)\n",
        "\n",
        "\n",
        "    with open(file) as f:\n",
        "        content = f.readlines()\n",
        "\n",
        "\n",
        "    for line in content:\n",
        "        tokens = re.split('\\s+', line)\n",
        "        if tokens[1] == '1':\n",
        "            # copy this image into target dir_path\n",
        "            target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n",
        "            src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n",
        "            shutil.copyfile(src_file_path, target_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4A69FhQhxvE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR=os.environ.get('DATA_DIR')\n",
        "SOURCE_DIR=os.path.join(DATA_DIR, 'formatted')\n",
        "TARGET_DIR=os.path.join(DATA_DIR,'split')\n",
        "# list dir\n",
        "print(os.walk(SOURCE_DIR))\n",
        "dir_list = next(os.walk(SOURCE_DIR))[1]\n",
        "# for each dir, create a new dir in split\n",
        "for dir_i in tqdm(dir_list):\n",
        "        newdir_train = os.path.join(TARGET_DIR, 'train', dir_i)\n",
        "        newdir_val = os.path.join(TARGET_DIR, 'val', dir_i)\n",
        "        newdir_test = os.path.join(TARGET_DIR, 'test', dir_i)\n",
        "\n",
        "        if not os.path.exists(newdir_train):\n",
        "                os.makedirs(newdir_train)\n",
        "        if not os.path.exists(newdir_val):\n",
        "                os.makedirs(newdir_val)\n",
        "        if not os.path.exists(newdir_test):\n",
        "                os.makedirs(newdir_test)\n",
        "\n",
        "        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n",
        "        # shuffle data\n",
        "        shuffle(img_list)\n",
        "\n",
        "        for j in range(int(len(img_list)*0.7)):\n",
        "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'train', dir_i))\n",
        "\n",
        "        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n",
        "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'val', dir_i))\n",
        "\n",
        "        for j in range(int(len(img_list)*0.8), len(img_list)):\n",
        "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'test', dir_i))\n",
        "\n",
        "print('Done splitting dataset.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o3Vc_8iPhxvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94da855c-4849-4f66-e22e-0b4e7916023c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2008_000060.jpg  2008_005853.jpg  2009_002104.jpg  2010_001712.jpg  2010_004144.jpg\n",
            "2008_000112.jpg  2008_005977.jpg  2009_002141.jpg  2010_001863.jpg  2010_004244.jpg\n",
            "2008_000536.jpg  2008_006113.jpg  2009_002352.jpg  2010_001885.jpg  2010_004335.jpg\n",
            "2008_000641.jpg  2008_006218.jpg  2009_002561.jpg  2010_001934.jpg  2010_004346.jpg\n",
            "2008_000724.jpg  2008_006280.jpg  2009_002704.jpg  2010_001939.jpg  2010_004365.jpg\n",
            "2008_000824.jpg  2008_006377.jpg  2009_002837.jpg  2010_002000.jpg  2010_004402.jpg\n",
            "2008_001004.jpg  2008_006384.jpg  2009_002972.jpg  2010_002025.jpg  2010_004479.jpg\n",
            "2008_001071.jpg  2008_006512.jpg  2009_003013.jpg  2010_002040.jpg  2010_004553.jpg\n",
            "2008_001357.jpg  2008_006576.jpg  2009_003415.jpg  2010_002086.jpg  2010_004584.jpg\n",
            "2008_001414.jpg  2008_006656.jpg  2009_003528.jpg  2010_002143.jpg  2010_004717.jpg\n",
            "2008_001433.jpg  2008_006746.jpg  2009_003601.jpg  2010_002333.jpg  2010_004816.jpg\n",
            "2008_001592.jpg  2008_006753.jpg  2009_003605.jpg  2010_002348.jpg  2010_004841.jpg\n",
            "2008_001640.jpg  2008_006843.jpg  2009_003654.jpg  2010_002406.jpg  2010_005099.jpg\n",
            "2008_002004.jpg  2008_006926.jpg  2009_003663.jpg  2010_002479.jpg  2010_005222.jpg\n",
            "2008_002067.jpg  2008_006999.jpg  2009_003771.jpg  2010_002485.jpg  2010_005275.jpg\n",
            "2008_002298.jpg  2008_007187.jpg  2009_003904.jpg  2010_002543.jpg  2010_005312.jpg\n",
            "2008_002329.jpg  2008_007216.jpg  2009_003955.jpg  2010_002678.jpg  2010_005573.jpg\n",
            "2008_002766.jpg  2008_007327.jpg  2009_003985.jpg  2010_002692.jpg  2010_005614.jpg\n",
            "2008_002872.jpg  2008_007548.jpg  2009_004176.jpg  2010_002763.jpg  2010_005696.jpg\n",
            "2008_003048.jpg  2008_007630.jpg  2009_004183.jpg  2010_002855.jpg  2010_005712.jpg\n",
            "2008_003228.jpg  2008_007726.jpg  2009_004456.jpg  2010_002909.jpg  2010_005732.jpg\n",
            "2008_003430.jpg  2008_007964.jpg  2009_004639.jpg  2010_002976.jpg  2010_005821.jpg\n",
            "2008_003659.jpg  2008_007975.jpg  2009_004779.jpg  2010_002993.jpg  2010_005855.jpg\n",
            "2008_003841.jpg  2009_000132.jpg  2009_004841.jpg  2010_003032.jpg  2010_005886.jpg\n",
            "2008_004074.jpg  2009_000150.jpg  2009_005083.jpg  2010_003067.jpg  2010_006040.jpg\n",
            "2008_004290.jpg  2009_000304.jpg  2010_000118.jpg  2010_003106.jpg  2011_000305.jpg\n",
            "2008_004328.jpg  2009_000342.jpg  2010_000157.jpg  2010_003151.jpg  2011_000379.jpg\n",
            "2008_004374.jpg  2009_000466.jpg  2010_000175.jpg  2010_003249.jpg  2011_000758.jpg\n",
            "2008_004515.jpg  2009_000553.jpg  2010_000244.jpg  2010_003275.jpg  2011_000769.jpg\n",
            "2008_004538.jpg  2009_000681.jpg  2010_000506.jpg  2010_003291.jpg  2011_000999.jpg\n",
            "2008_004635.jpg  2009_000862.jpg  2010_000519.jpg  2010_003337.jpg  2011_001323.jpg\n",
            "2008_004732.jpg  2009_001008.jpg  2010_000536.jpg  2010_003467.jpg  2011_001369.jpg\n",
            "2008_004966.jpg  2009_001094.jpg  2010_000576.jpg  2010_003468.jpg  2011_001754.jpg\n",
            "2008_004967.jpg  2009_001153.jpg  2010_000586.jpg  2010_003641.jpg  2011_001775.jpg\n",
            "2008_004990.jpg  2009_001270.jpg  2010_000616.jpg  2010_003672.jpg  2011_001911.jpg\n",
            "2008_005003.jpg  2009_001361.jpg  2010_000941.jpg  2010_003752.jpg  2011_002409.jpg\n",
            "2008_005288.jpg  2009_001369.jpg  2010_000996.jpg  2010_003861.jpg  2011_002664.jpg\n",
            "2008_005463.jpg  2009_001411.jpg  2010_001021.jpg  2010_003863.jpg  2011_002724.jpg\n",
            "2008_005469.jpg  2009_001444.jpg  2010_001195.jpg  2010_003875.jpg  2011_002916.jpg\n",
            "2008_005510.jpg  2009_001450.jpg  2010_001401.jpg  2010_003898.jpg  2011_003159.jpg\n",
            "2008_005566.jpg  2009_001522.jpg  2010_001418.jpg  2010_003928.jpg\n",
            "2008_005569.jpg  2009_001835.jpg  2010_001465.jpg  2010_003943.jpg\n",
            "2008_005600.jpg  2009_001948.jpg  2010_001516.jpg  2010_004048.jpg\n",
            "2008_005780.jpg  2009_002008.jpg  2010_001660.jpg  2010_004104.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls $DATA_DIR/split/test/cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-rdo1SohxvF"
      },
      "source": [
        "### B. Download pretrained models <a class=\"anchor\" id=\"head-2-2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnHXYYw0hxvF"
      },
      "source": [
        " We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VLIhbgXhxvF",
        "outputId": "40682a02-044d-4931-ccc1-c4cfb8444676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LOCAL_PROJECT_DIR=/ngc_content/\n",
            "env: CLI=ngccli_cat_linux.zip\n",
            "--2024-11-30 16:06:52--  https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip\n",
            "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 52.88.116.192, 44.239.44.161\n",
            "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|52.88.116.192|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://xfiles.ngc.nvidia.com/org/nvidia/team/ngc-apps/recipes/ngc_cli/versions/3.23.0/files/ngccli_linux.zip?versionId=Bpzrduq29jxiO6V_pwHtxB_RuGz7cqzb&Expires=1733069213&Signature=xzmXwvkNiw4Qxdz8GL3Aec0wP2iJP0EojFab8LCN7TYeK6XVxyl0fwbR69yKVPWupf8DWvxQVSGR4GCSFYerdvwkl0yfoJ-W9wXk7VQpcX~lLzpYqHno155fxcNqcdRuJFbo4r4yA7DKdNrtN4TmDY4ms5h5mSJEKg~FoQgKifqyAofLQSf1F70OEOZULLIGDOJEbmcCraQEUqXAr-8ZbC5b101ec7TB1Ha2WOQbE0lDKkWa5BSTQxeSToHthAOnVo0HjGf8Oh8P7eHgkrvQ3R6X2Oop6jFFuaWzas~t8eJfMXl7DPfqEx41DoXW0t-ow1PRijYU4xG9Lf3tZGpASw__&Key-Pair-Id=KCX06E8E9L60W [following]\n",
            "--2024-11-30 16:06:53--  https://xfiles.ngc.nvidia.com/org/nvidia/team/ngc-apps/recipes/ngc_cli/versions/3.23.0/files/ngccli_linux.zip?versionId=Bpzrduq29jxiO6V_pwHtxB_RuGz7cqzb&Expires=1733069213&Signature=xzmXwvkNiw4Qxdz8GL3Aec0wP2iJP0EojFab8LCN7TYeK6XVxyl0fwbR69yKVPWupf8DWvxQVSGR4GCSFYerdvwkl0yfoJ-W9wXk7VQpcX~lLzpYqHno155fxcNqcdRuJFbo4r4yA7DKdNrtN4TmDY4ms5h5mSJEKg~FoQgKifqyAofLQSf1F70OEOZULLIGDOJEbmcCraQEUqXAr-8ZbC5b101ec7TB1Ha2WOQbE0lDKkWa5BSTQxeSToHthAOnVo0HjGf8Oh8P7eHgkrvQ3R6X2Oop6jFFuaWzas~t8eJfMXl7DPfqEx41DoXW0t-ow1PRijYU4xG9Lf3tZGpASw__&Key-Pair-Id=KCX06E8E9L60W\n",
            "Resolving xfiles.ngc.nvidia.com (xfiles.ngc.nvidia.com)... 108.156.133.98, 108.156.133.102, 108.156.133.13, ...\n",
            "Connecting to xfiles.ngc.nvidia.com (xfiles.ngc.nvidia.com)|108.156.133.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43271011 (41M) [binary/octet-stream]\n",
            "Saving to: ‘/ngc_content//ngccli/ngccli_cat_linux.zip’\n",
            "\n",
            "ngccli_cat_linux.zi 100%[===================>]  41.27M  13.6MB/s    in 3.0s    \n",
            "\n",
            "2024-11-30 16:06:57 (13.6 MB/s) - ‘/ngc_content//ngccli/ngccli_cat_linux.zip’ saved [43271011/43271011]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Installing NGC CLI on the local machine.\n",
        "## Download and install\n",
        "%env LOCAL_PROJECT_DIR=/ngc_content/\n",
        "%env CLI=ngccli_cat_linux.zip\n",
        "!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n",
        "\n",
        "# Remove any previously existing CLI installations\n",
        "!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
        "!wget --content-disposition 'https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip' -P $LOCAL_PROJECT_DIR/ngccli -O $LOCAL_PROJECT_DIR/ngccli/$CLI\n",
        "!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
        "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip\n",
        "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
        "!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LMxzryhxvF",
        "outputId": "18c2cdbc-69b3-44b5-d2b6-e6a6ff8e8957",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI_VERSION: Latest - 3.55.0 available (current: 3.23.0). Please update by using the command 'ngc version upgrade' \n",
            "\n",
            "+----------+----------+--------+-------+-------+----------+----------+----------+---------+\n",
            "| Version  | Accuracy | Epochs | Batch | GPU   | Memory F | File     | Status   | Created |\n",
            "|          |          |        | Size  | Model | ootprint | Size     |          | Date    |\n",
            "+----------+----------+--------+-------+-------+----------+----------+----------+---------+\n",
            "| vgg19    | 77.56    | 80     | 1     | V100  | 153.7    | 153.72   | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| vgg16    | 77.17    | 80     | 1     | V100  | 113.2    | 113.16   | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| squeezen | 65.13    | 80     | 1     | V100  | 6.5      | 6.46 MB  | UPLOAD_C | Aug 18, |\n",
            "| et       |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| resnet50 | 77.91    | 80     | 1     | V100  | 294.2    | 294.2 MB | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| resnet34 | 77.04    | 80     | 1     | V100  | 170.7    | 170.65   | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| resnet18 | 76.74    | 80     | 1     | V100  | 89.0     | 88.96 MB | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| resnet10 | 77.78    | 80     | 1     | V100  | 576.3    | 576.33   | UPLOAD_C | Aug 18, |\n",
            "| 1        |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| resnet10 | 74.38    | 80     | 1     | V100  | 38.3     | 38.31 MB | UPLOAD_C | Aug 18, |\n",
            "|          |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| mobilene | 72.75    | 80     | 1     | V100  | 5.0      | 5.01 MB  | UPLOAD_C | Aug 18, |\n",
            "| t_v2     |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| mobilene | 79.5     | 80     | 1     | V100  | 26.2     | 26.22 MB | UPLOAD_C | Aug 18, |\n",
            "| t_v1     |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| googlene | 77.11    | 80     | 1     | V100  | 47.6     | 47.64 MB | UPLOAD_C | Aug 18, |\n",
            "| t        |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| efficien | 77.11    | 80     | 1     | V100  | 26.8     | 26.78 MB | UPLOAD_C | Aug 18, |\n",
            "| tnet_b1_ |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| swish    |          |        |       |       |          |          |          |         |\n",
            "| efficien | 77.11    | 80     | 1     | V100  | 26.8     | 26.78 MB | UPLOAD_C | Aug 18, |\n",
            "| tnet_b1_ |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| relu     |          |        |       |       |          |          |          |         |\n",
            "| darknet5 | 76.44    | 80     | 1     | V100  | 311.7    | 311.68   | UPLOAD_C | Aug 18, |\n",
            "| 3        |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| darknet1 | 77.52    | 80     | 1     | V100  | 152.8    | 152.82   | UPLOAD_C | Aug 18, |\n",
            "| 9        |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| cspdarkn | 77.1     | 80     | 1     | V100  | 28.6     | 28.57 MB | UPLOAD_C | Nov 23, |\n",
            "| et_tiny  |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "| cspdarkn | 76.44    | 80     | 1     | V100  | 103.0    | 102.99   | UPLOAD_C | Sep 10, |\n",
            "| et53     |          |        |       |       |          | MB       | OMPLETE  | 2021    |\n",
            "| cspdarkn | 77.52    | 80     | 1     | V100  | 62.9     | 62.86 MB | UPLOAD_C | Sep 10, |\n",
            "| et19     |          |        |       |       |          |          | OMPLETE  | 2021    |\n",
            "+----------+----------+--------+-------+-------+----------+----------+----------+---------+\n"
          ]
        }
      ],
      "source": [
        "!ngc registry model list nvidia/tao/pretrained_classification:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6Swj3isyhxvF"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qopJpwV_hxvG",
        "outputId": "6d73a8c0-83c7-4bad-c947-762a8a657250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting files to download...\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/89…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.0/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:06:…\u001b[0m • \u001b[31m225.7\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.0/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:06:…\u001b[0m • \u001b[31m225.7\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.1/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:04:…\u001b[0m • \u001b[31m386.5\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:02:…\u001b[0m • \u001b[31m623.2\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:01:…\u001b[0m • \u001b[31m991.8\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:01:…\u001b[0m • \u001b[31m963.6\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m1.3/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m1.6  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m2.6/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m2.8  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m2.6/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m2.6  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m5.1/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m4.7  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m8.6/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m6.9  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m9.0/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m7.2  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m12.8…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m9.2  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m16.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m10.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m16.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m10.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m20.5…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m12.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m24.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m13.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m24.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m13.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m28.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m13.9 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m31.9…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m14.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m32.6…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m14.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m35.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m15.4 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m39.5…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m16.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m41.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m16.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m43.4…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m16.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m47.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m17.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m50.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m17.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m51.0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m17.4 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m54.9…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m17.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m58.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m22.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m58.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m22.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m62.5…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m66.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m66.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m70.0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m73.9…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m73.9…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m77.6…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m81.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[33m╸\u001b[0m • \u001b[32m82.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m24.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[33m╸\u001b[0m • \u001b[32m85.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K  \u001b[90m━━━━━━\u001b[0m • \u001b[32m89.0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m25.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 1 - Completed: 1 - Failed: 0\u001b[0m\n",
            "           \u001b[32mMiB  \u001b[0m                       \u001b[31mMB/s \u001b[0m                                                        \n",
            "\u001b[?25h\n",
            "----------------------------------------------------------------------------------------------------\n",
            "   Download status: COMPLETED\n",
            "   Downloaded local path model: /content/drive/MyDrive/TAO_Hiwi/results/classification/pretrained_resnet18/pretrained_classification_vresnet18\n",
            "   Total files downloaded: 1\n",
            "   Total transferred: 88.96 MB\n",
            "   Started at: 2024-11-30 16:07:43\n",
            "   Completed at: 2024-11-30 16:07:50\n",
            "   Duration taken: 6s\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Pull pretrained model from NGC\n",
        "!ngc registry model download-version nvidia/tao/pretrained_classification:resnet18 --dest $EXPERIMENT_DIR/pretrained_resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dCQQa0NhxvG",
        "outputId": "db449318-8d00-41bf-92d4-61be1914fbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check that model is downloaded into dir.\n",
            "total 91093\n",
            "-rw------- 1 root root 93278448 Nov 30 16:07 resnet_18.hdf5\n"
          ]
        }
      ],
      "source": [
        "print(\"Check that model is downloaded into dir.\")\n",
        "!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_classification_vresnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_26rCobXcri1"
      },
      "source": [
        "## 2. Setup GPU environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBV_YWiTc_KM"
      },
      "source": [
        "### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n",
        "Setup the environment necessary to run the TAO Networks by running the bash script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vij05HmUIfrN",
        "outputId": "989f144d-9fec-40f6-d355-617d94ce18d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorRT tarball found at: /content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\n",
            "Write access to /content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/trt_untar confirmed.\n",
            "Extracting TensorRT to: /content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/trt_untar\n",
            "TensorRT extraction complete and library path updated.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###prev code\n",
        "# FIXME 7: set this path of the uploaded TensorRT tar.gz file after browser download\n",
        "# trt_tar_path=\"/content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\"\n",
        "\n",
        "# import os\n",
        "# if not os.path.exists(trt_tar_path):\n",
        "#   raise Exception(\"TAR file not found in the provided path\")\n",
        "\n",
        "# # Ensure directory exists\n",
        "# if not os.path.exists(trt_untar_folder_path):\n",
        "#     print(f\"Creating directory {trt_untar_folder_path}\")\n",
        "#     os.makedirs(trt_untar_folder_path, exist_ok=True)\n",
        "\n",
        "# # Verify that you have write permissions\n",
        "# if os.access(trt_untar_folder_path, os.W_OK):\n",
        "#     print(f\"Write access to {trt_untar_folder_path} confirmed.\")\n",
        "# else:\n",
        "#     print(f\"No write access to {trt_untar_folder_path}. Please check your permissions.\")\n",
        "\n",
        "# # FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n",
        "# #//Commented due to issues\n",
        "# # %env trt_untar_folder_path=/content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/trt_untar\n",
        "# trt_untar_folder_path = \"/content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/trt_untar\"\n",
        "\n",
        "\n",
        "# # FIXME 9: set this to the version of TRT you have downloaded\n",
        "# %env trt_version=8.6.1.6\n",
        "\n",
        "# !sudo mkdir -p $trt_untar_folder_path && sudo chmod -R 777 $trt_untar_folder_path/\n",
        "\n",
        "# import os\n",
        "\n",
        "# untar = True\n",
        "# for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n",
        "#   if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n",
        "#     untar = False\n",
        "\n",
        "# if untar:\n",
        "#   !tar -xzf $trt_tar_path -C /content/trt_untar\n",
        "\n",
        "# if os.environ.get(\"LD_LIBRARY_PATH\",\"\") == \"\":\n",
        "#   os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n",
        "# trt_lib_path = f':{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}/lib'\n",
        "# os.environ[\"LD_LIBRARY_PATH\"]+=trt_lib_path\n",
        "######## PREVIOUS CODE####\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the path for TensorRT tarball and the untar folder\n",
        "trt_tar_path = \"/content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\"\n",
        "trt_untar_folder_path = \"/content/drive/MyDrive/TAO_Hiwi/linux_tensor_rt/trt_untar\"\n",
        "\n",
        "# Check if the tarball file exists\n",
        "if not os.path.exists(trt_tar_path):\n",
        "    raise Exception(f\"TAR file not found at {trt_tar_path}\")\n",
        "\n",
        "print(f\"TensorRT tarball found at: {trt_tar_path}\")\n",
        "\n",
        "# Ensure the untar folder exists and is writable\n",
        "if not os.path.exists(trt_untar_folder_path):\n",
        "    print(f\"Creating directory {trt_untar_folder_path}\")\n",
        "    os.makedirs(trt_untar_folder_path, exist_ok=True)\n",
        "\n",
        "# Verify write permissions\n",
        "if os.access(trt_untar_folder_path, os.W_OK):\n",
        "    print(f\"Write access to {trt_untar_folder_path} confirmed.\")\n",
        "else:\n",
        "    raise PermissionError(f\"No write access to {trt_untar_folder_path}. Please check permissions.\")\n",
        "\n",
        "# Check if TensorRT is already extracted, if not, extract it\n",
        "untar = True\n",
        "for fname in os.listdir(trt_untar_folder_path):\n",
        "    if fname.startswith(\"TensorRT-\" + \"8.6.1.6\") and not fname.endswith(\".tar.gz\"):\n",
        "        untar = False\n",
        "\n",
        "if untar:\n",
        "    print(f\"Extracting TensorRT to: {trt_untar_folder_path}\")\n",
        "    # Use the Python variable directly in the tar command\n",
        "    !sudo tar -xzf {trt_tar_path} -C {trt_untar_folder_path}\n",
        "\n",
        "# Update LD_LIBRARY_PATH to include TensorRT libraries\n",
        "if os.environ.get(\"LD_LIBRARY_PATH\", \"\") == \"\":\n",
        "    os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n",
        "trt_lib_path = f':{trt_untar_folder_path}/TensorRT-8.6.1.6/lib'\n",
        "os.environ[\"LD_LIBRARY_PATH\"] += trt_lib_path\n",
        "\n",
        "print(\"TensorRT extraction complete and library path updated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s2Xygw-y8fjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756c5813-7a30-481d-f221-64986708c89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-30 16:19:54--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4328 (4.2K) [application/x-deb]\n",
            "Saving to: ‘cuda-keyring_1.0-1_all.deb’\n",
            "\n",
            "\r          cuda-keyr   0%[                    ]       0  --.-KB/s               \rcuda-keyring_1.0-1_ 100%[===================>]   4.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-30 16:19:54 (267 MB/s) - ‘cuda-keyring_1.0-1_all.deb’ saved [4328/4328]\n",
            "\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack cuda-keyring_1.0-1_all.deb ...\n",
            "Unpacking cuda-keyring (1.0-1) over (1.0-1) ...\n",
            "Setting up cuda-keyring (1.0-1) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,866 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,515 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,621 kB]\n",
            "Fetched 21.4 MB in 4s (5,329 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Some packages could not be installed. This may mean that you have\n",
            "requested an impossible situation or if you are using the unstable\n",
            "distribution that some required packages have not yet been created\n",
            "or been moved out of Incoming.\n",
            "The following information may help to resolve the situation:\n",
            "\n",
            "The following packages have unmet dependencies:\n",
            " cuda-drivers-565 : Depends: nvidia-driver-565 (>= 565.57.01) but it is not installable or\n",
            "                             nvidia-driver-565-open (>= 565.57.01) but it is not installable or\n",
            "                             nvidia-driver-565-server (>= 565.57.01) but it is not installable or\n",
            "                             nvidia-driver-565-server-open (>= 565.57.01) but it is not installable\n",
            "E: Unable to correct problems, you have held broken packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/ jammy main'\n",
            "Description:\n",
            "This PPA contains more recent Python versions packaged for Ubuntu.\n",
            "\n",
            "Disclaimer: there's no guarantee of timely updates in case of security problems or other issues. If you want to use them in a security-or-otherwise-critical environment (say, on a production server), you do so at your own risk.\n",
            "\n",
            "Update Note\n",
            "===========\n",
            "Please use this repository instead of ppa:fkrull/deadsnakes.\n",
            "\n",
            "Reporting Issues\n",
            "================\n",
            "\n",
            "Issues can be reported in the master issue tracker at:\n",
            "https://github.com/deadsnakes/issues/issues\n",
            "\n",
            "Supported Ubuntu and Python Versions\n",
            "====================================\n",
            "\n",
            "- Ubuntu 20.04 (focal) Python3.5 - Python3.7, Python3.9 - Python3.13\n",
            "- Ubuntu 22.04 (jammy) Python3.7 - Python3.9, Python3.11 - Python3.13\n",
            "- Ubuntu 24.04 (noble) Python3.7 - Python3.11, Python3.13\n",
            "- Note: Python2.7 (focal, jammy), Python 3.8 (focal), Python 3.10 (jammy), Python3.12 (noble) are not provided by deadsnakes as upstream ubuntu provides those packages.\n",
            "\n",
            "Why some packages aren't built:\n",
            "- Note: for focal, older python versions require libssl<1.1 so they are not currently built\n",
            "- Note: for jammy and noble, older python versions requre libssl<3 so they are not currently built\n",
            "- If you need these, reach out to asottile to set up a private ppa\n",
            "\n",
            "The packages may also work on other versions of Ubuntu or Debian, but that is not tested or supported.\n",
            "\n",
            "Packages\n",
            "========\n",
            "\n",
            "The packages provided here are loosely based on the debian upstream packages with some modifications to make them more usable as non-default pythons and on ubuntu.  As such, the packages follow debian's patterns and often do not include a full python distribution with just `apt install python#.#`.  Here is a list of packages that may be useful along with the default install:\n",
            "\n",
            "- `python#.#-dev`: includes development headers for building C extensions\n",
            "- `python#.#-venv`: provides the standard library `venv` module\n",
            "- `python#.#-distutils`: provides the standard library `distutils` module\n",
            "- `python#.#-lib2to3`: provides the `2to3-#.#` utility as well as the standard library `lib2to3` module\n",
            "- `python#.#-gdbm`: provides the standard library `dbm.gnu` module\n",
            "- `python#.#-tk`: provides the standard library `tkinter` module\n",
            "\n",
            "Third-Party Python Modules\n",
            "==========================\n",
            "\n",
            "Python modules in the official Ubuntu repositories are packaged to work with the Python interpreters from the official repositories. Accordingly, they generally won't work with the Python interpreters from this PPA. As an exception, pure-Python modules for Python 3 will work, but any compiled extension modules won't.\n",
            "\n",
            "To install 3rd-party Python modules, you should use the common Python packaging tools.  For an introduction into the Python packaging ecosystem and its tools, refer to the Python Packaging User Guide:\n",
            "https://packaging.python.org/installing/\n",
            "\n",
            "Sources\n",
            "=======\n",
            "The package sources are available at:\n",
            "https://github.com/deadsnakes/\n",
            "\n",
            "Nightly Builds\n",
            "==============\n",
            "\n",
            "For nightly builds, see ppa:deadsnakes/nightly https://launchpad.net/~deadsnakes/+archive/ubuntu/nightly\n",
            "More info: https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa\n",
            "Adding repository.\n",
            "Found existing deb entry in /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
            "Adding deb entry to /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
            "Found existing deb-src entry in /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/deadsnakes-ubuntu-ppa-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/deadsnakes-ubuntu-ppa.gpg with fingerprint F23C5A6CF475977595C89F51BA6932366A755776\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support\n",
            "  python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8\n",
            "  python3.8-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 5,103 kB of archives.\n",
            "After this operation, 18.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.20-1+jammy1 [796 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.20-1+jammy1 [2,023 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.20-1+jammy1 [1,817 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.20-1+jammy1 [440 kB]\n",
            "Fetched 5,103 kB in 2s (2,270 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 123631 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../1-python3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.8-stdlib_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../5-python3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-minimal (3.8.20-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.20-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,968 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.2 [340 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Fetched 1,677 kB in 2s (809 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 124282 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.8-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.8-distutils python3.8-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 319 kB of archives.\n",
            "After this operation, 1,237 kB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.20-1+jammy1 [126 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.20-1+jammy1 [193 kB]\n",
            "Fetched 319 kB in 2s (184 kB/s)\n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "(Reading database ... 125144 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.8-lib2to3_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../python3.8-distutils_3.8.20-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.20-1+jammy1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8 libpython3.8-dev\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8 libpython3.8-dev python3.8-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 6,687 kB of archives.\n",
            "After this operation, 25.0 MB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8 amd64 3.8.20-1+jammy1 [1,798 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-dev amd64 3.8.20-1+jammy1 [4,389 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-dev amd64 3.8.20-1+jammy1 [500 kB]\n",
            "Fetched 6,687 kB in 3s (2,574 kB/s)\n",
            "Selecting previously unselected package libpython3.8:amd64.\n",
            "(Reading database ... 125283 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.8_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.8-dev:amd64.\n",
            "Preparing to unpack .../libpython3.8-dev_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-dev:amd64 (3.8.20-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-dev.\n",
            "Preparing to unpack .../python3.8-dev_3.8.20-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-dev (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up libpython3.8-dev:amd64 (3.8.20-1+jammy1) ...\n",
            "Setting up python3.8-dev (3.8.20-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "rm: cannot remove '/usr/bin/python': No such file or directory\n",
            "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-24.3.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pip<24.1\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.3.1\n",
            "    Uninstalling pip-24.3.1:\n",
            "      Successfully uninstalled pip-24.3.1\n",
            "Successfully installed pip-24.0\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8416 sha256=909c3026e135a1bbdbb53b0e24734ad34f497aae0e2a80c2b6da9283a7bc1480\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "Collecting cython==0.29.35\n",
            "  Downloading Cython-0.29.35-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Downloading Cython-0.29.35-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cython\n",
            "Successfully installed cython-0.29.35\n",
            "Collecting pycocotools\n",
            "  Downloading pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting matplotlib>=2.1.0 (from pycocotools)\n",
            "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting numpy (from pycocotools)\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading fonttools-4.55.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/164.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pillow>=6.2.0 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools)\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Downloading pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.55.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: zipp, python-dateutil, pillow, packaging, numpy, kiwisolver, fonttools, cycler, importlib-resources, contourpy, matplotlib, pycocotools\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 1.0.0\n",
            "    Uninstalling zipp-1.0.0:\n",
            "      Successfully uninstalled zipp-1.0.0\n",
            "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.55.0 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.7.5 numpy-1.24.4 packaging-24.2 pillow-10.4.0 pycocotools-2.0.7 python-dateutil-2.9.0.post0 zipp-3.20.2\n",
            "Collecting pycocotools-fix==2.0.0.9\n",
            "  Downloading pycocotools-fix-2.0.0.9.tar.gz (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.8/dist-packages (from pycocotools-fix==2.0.0.9) (0.29.35)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools-fix==2.0.0.9) (3.7.5)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/lib/python3/dist-packages (from pycocotools-fix==2.0.0.9) (59.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (6.4.5)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (3.20.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools-fix==2.0.0.9) (1.16.0)\n",
            "Building wheels for collected packages: pycocotools-fix\n",
            "  Building wheel for pycocotools-fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools-fix: filename=pycocotools_fix-2.0.0.9-cp38-cp38-linux_x86_64.whl size=373946 sha256=42cb217af6c2c90c5c494964b2aa05b61a8eb9e1e2b842cf709695e3009eb1ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/44/fb/0194f0a86b9f927546ae5e6e4c964cda0e7ec1504f3e21f21e\n",
            "Successfully built pycocotools-fix\n",
            "Installing collected packages: pycocotools-fix\n",
            "Successfully installed pycocotools-fix-2.0.0.9\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.8/dist-packages (from h5py==2.10.0) (1.24.4)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from h5py==2.10.0) (1.16.0)\n",
            "Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h5py\n",
            "Successfully installed h5py-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Collecting nvidia-tensorflow==1.15.5+nv23.2\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-tensorflow/nvidia_tensorflow-1.15.5%2Bnv23.02-7195399-cp38-cp38-linux_x86_64.whl (397.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.9/397.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.9.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astunparse==1.6.3 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting astor==0.8.1 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting gast==0.3.3 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta>=0.1.6 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting keras-applications>=1.0.8 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting keras-preprocessing>=1.0.5 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from nvidia-tensorflow==1.15.5+nv23.2) (1.16.0)\n",
            "Collecting protobuf<4.0.0,>=3.6.1 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/tensorboard/tensorboard-1.15.0-py2.py3-none-any.whl (1.6 kB)\n",
            "Collecting tensorflow-estimator==1.15.1 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting termcolor>=1.1.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wrapt>=1.11.1 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading wrapt-1.17.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.8/dist-packages (from nvidia-tensorflow==1.15.5+nv23.2) (2.10.0)\n",
            "Collecting grpcio>=1.8.6 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading grpcio-1.68.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12~=12.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cublas-cu12~=12.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12~=11.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu11~=8.7 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.9.6.50-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-curand-cu12~=10.3 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12~=11.4 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12~=12.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12~=2.16 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12~=12.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvcc-cu12~=12.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.6.85-py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-dali-nvtf-plugin==1.22.0+nv23.02 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-nvtf-plugin/nvidia_dali_nvtf_plugin-1.22.0%2Bnv23.02-7195399-cp38-cp38-linux_x86_64.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.2/127.2 kB\u001b[0m \u001b[31m543.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorrt~=8.5 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from nvidia-tensorflow==1.15.5+nv23.2) (0.37.1)\n",
            "Collecting numpy<1.24,>=1.22.0 (from nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting nvidia-dali-cuda110==1.22.0 (from nvidia-dali-nvtf-plugin==1.22.0+nv23.02->nvidia-tensorflow==1.15.5+nv23.2)\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.22.0-6988993-py3-none-manylinux2014_x86_64.whl (480.1 MB)\n",
            "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/480.1 MB\u001b[0m \u001b[31m198.7 kB/s\u001b[0m eta \u001b[36m0:39:28\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    os.environ[\"bash_script\"] = \"setup_env.sh\"\n",
        "else:\n",
        "    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n",
        "\n",
        "os.environ[\"NV_TAO_TF_TOP\"] = \"/tmp/tao_tensorflow1_backend/\"\n",
        "\n",
        "!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "\n",
        "!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC7BeDZshxvG"
      },
      "source": [
        "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
        "* Training dataset\n",
        "* Validation dataset\n",
        "* Pre-trained models\n",
        "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n"
      ],
      "metadata": {
        "id": "Pp7bn5JjW8X-",
        "outputId": "23c83e79-a788-436c-e055-e09e7e55f804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (1.15.0)\n",
            "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dbe4zB_hxvG",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404dbd11-20ee-4c84-c886-98179f34d7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_config {\n",
            "  arch: \"resnet\",\n",
            "  n_layers: 18\n",
            "  # Setting these parameters to true to match the template downloaded from NGC.\n",
            "  use_batch_norm: true\n",
            "  all_projections: true\n",
            "  freeze_blocks: 0\n",
            "  freeze_blocks: 1\n",
            "  input_image_size: \"3,224,224\"\n",
            "}\n",
            "train_config {\n",
            "  train_dataset_path: \"/content/drive/MyDrive/TAO_Hiwi/data_images//split/train\"\n",
            "  val_dataset_path: \"/content/drive/MyDrive/TAO_Hiwi/data_images//split/val\"\n",
            "  pretrained_model_path: \"/content/drive/MyDrive/TAO_Hiwi/results/classification//pretrained_resnet18/pretrained_classification_vresnet18/resnet_18.hdf5\"\n",
            "  optimizer {\n",
            "    sgd {\n",
            "    lr: 0.01\n",
            "    decay: 0.0\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "  }\n",
            "}\n",
            "  batch_size_per_gpu: 64\n",
            "  n_epochs: 10\n",
            "  n_workers: 16\n",
            "  preprocess_mode: \"caffe\"\n",
            "  enable_random_crop: True\n",
            "  enable_center_crop: True\n",
            "  label_smoothing: 0.0\n",
            "  mixup_alpha: 0.1\n",
            "  # regularizer\n",
            "  reg_config {\n",
            "    type: \"L2\"\n",
            "    scope: \"Conv2D,Dense\"\n",
            "    weight_decay: 0.00005\n",
            "  }\n",
            "\n",
            "  # learning_rate\n",
            "  lr_config {\n",
            "    step {\n",
            "      learning_rate: 0.006\n",
            "      step_size: 10\n",
            "      gamma: 0.1\n",
            "    }\n",
            "  }\n",
            "}\n",
            "eval_config {\n",
            "  eval_dataset_path: \"/content/drive/MyDrive/TAO_Hiwi/data_images//split/test\"\n",
            "  model_path: \"/content/drive/MyDrive/TAO_Hiwi/results/classification//output/weights/resnet_010.hdf5\"\n",
            "  top_k: 3\n",
            "  batch_size: 256\n",
            "  n_workers: 8\n",
            "  enable_center_crop: True\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n",
        "!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n",
        "!cat $SPECS_DIR/classification_spec.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_63FqBhxvG"
      },
      "source": [
        "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
        "* Provide the sample spec file and the output directory location for models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79_fDoqxhxvG",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95a7c19-9cd4-468a-e823-7e63ad2f67fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2024-11-30 13:56:43.037783: I tensorflow/stream_executor/platform/default/dso_loader.cc:50] Successfully opened dynamic library libcudart.so.12\n",
            "2024-11-30 13:56:43,091 [TAO Toolkit] [WARNING] tensorflow 40: Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
            "2024-11-30 13:56:44,251 [TAO Toolkit] [WARNING] root 329: Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/classification_tf1\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/cv/makenet/entrypoint/makenet.py\", line 12, in main\n",
            "    launch_job(nvidia_tao_tf1.cv.makenet.scripts, \"classification\", sys.argv[1:])\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/cv/common/entrypoint/entrypoint.py\", line 276, in launch_job\n",
            "    modules = get_modules(package)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/cv/common/entrypoint/entrypoint.py\", line 47, in get_modules\n",
            "    module = importlib.import_module(module_name)\n",
            "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/cv/makenet/scripts/train.py\", line 33, in <module>\n",
            "    from nvidia_tao_tf1.core.utils import set_random_seed\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/core/__init__.py\", line 19, in <module>\n",
            "    from nvidia_tao_tf1.core import export\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/core/export/__init__.py\", line 23, in <module>\n",
            "    from nvidia_tao_tf1.core.export._onnx import keras_to_onnx\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/core/export/_onnx.py\", line 30, in <module>\n",
            "    from nvidia_tao_tf1.core.export._uff import _reload_model_for_inference\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/nvidia_tao_tf1/core/export/_uff.py\", line 34, in <module>\n",
            "    import uff\n",
            "ModuleNotFoundError: No module named 'uff'\n"
          ]
        }
      ],
      "source": [
        "!tao model classification_tf1 train -e $SPECS_DIR/classification_spec.cfg -r $EXPERIMENT_DIR/output -k $KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpu0CH5ShxvH"
      },
      "outputs": [],
      "source": [
        "print(\"To run this training in data parallelism using multiple GPU's, please uncomment the line below and \"\n",
        "      \"update the --gpus parameter to the number of GPU's you wish to use.\")\n",
        "# !tao model classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n",
        "#                       -r $EXPERIMENT_DIR/output \\\n",
        "#                       -k $KEY --gpus 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEaTcEKihxvI"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "      To run this training in model parallelism using multiple GPU's, please uncomment the line below and update the\n",
        "      --gpus parameter to the number of GPU's you wish to use. Also add related parameters in training_config to\n",
        "      enable model parallelism. E.g.,\n",
        "\n",
        "             model_parallelism: 50\n",
        "             model_parallelism: 50\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "#!tao model classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n",
        "#                       -r $EXPERIMENT_DIR/output \\\n",
        "#                       -k $KEY --gpus 2 \\\n",
        "#                       -np 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc1WvRYJhxvK"
      },
      "outputs": [],
      "source": [
        "print(\"To resume from a checkpoint, use --init_epoch along with your checkpoint configured in the spec file.\")\n",
        "print(\"Please make sure that the model_path in the spec file is now updated to the '.tlt' file of the corresponding\"\n",
        "      \"epoch you wish to resume from. You may choose from the files found under, '$EXPERIMENT_DIR/output/weights' folder.\")\n",
        "# !tao model classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n",
        "#                        -r $EXPERIMENT_DIR/output \\\n",
        "#                        -k $KEY --gpus 2 \\\n",
        "#                        --init_epoch N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE0cOeL3hxvL"
      },
      "source": [
        "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>\n",
        "\n",
        "In this step, we assume that the training is complete and the model from the final epoch (`resnet_010.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l0J-JmWhxvL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!tao model classification_tf1 evaluate -e $SPECS_DIR/classification_spec.cfg -k $KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD0t8fkwhxvL"
      },
      "source": [
        "## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n",
        "* Specify pre-trained model\n",
        "* Equalization criterion\n",
        "* Threshold for pruning\n",
        "* Exclude prediction layer that you don't want pruned (e.g. predictions)\n",
        "\n",
        "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A pth value 0.68 is just a starting point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjnvpJREhxvL"
      },
      "outputs": [],
      "source": [
        "# Defining the checkpoint epoch number of the model to be used for the pruning.\n",
        "# This should be lesser than the number of epochs training has been run for, in case training was interrupted earlier.\n",
        "# By default, the default final model is at epoch 010.\n",
        "%env EPOCH=010\n",
        "!mkdir -p $EXPERIMENT_DIR/output/resnet_pruned\n",
        "!tao model classification_tf1 prune -m $EXPERIMENT_DIR/output/weights/resnet_$EPOCH.hdf5 \\\n",
        "           -o $EXPERIMENT_DIR/output/resnet_pruned/resnet18_nopool_bn_pruned.hdf5 \\\n",
        "           -eq union \\\n",
        "           -pth 0.6 \\\n",
        "           -k $KEY \\\n",
        "           --results_dir $EXPERIMENT_DIR/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgcs8JuDhxvM"
      },
      "outputs": [],
      "source": [
        "print('Pruned model:')\n",
        "print('------------')\n",
        "!ls -rlt $EXPERIMENT_DIR/output/resnet_pruned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWKjLHjhxvM"
      },
      "source": [
        "## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n",
        "* Model needs to be re-trained to bring back accuracy after pruning\n",
        "* Specify re-training specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs8HDEwshxvM"
      },
      "outputs": [],
      "source": [
        "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n",
        "!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n",
        "!cat $SPECS_DIR/classification_retrain_spec.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XceCQKQzhxvN"
      },
      "outputs": [],
      "source": [
        "!tao model classification_tf1 train -e $SPECS_DIR/classification_retrain_spec.cfg \\\n",
        "                      -r $EXPERIMENT_DIR/output_retrain \\\n",
        "                      -k $KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZhkoOEhxvN"
      },
      "source": [
        "## 8. Testing the model! <a class=\"anchor\" id=\"head-8\"></a>\n",
        "\n",
        "In this step, we assume that the training is complete and the model from the final epoch (`resnet_010.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_retrain_spec.cfg` to point to the intended model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XGYkITuhxvN"
      },
      "outputs": [],
      "source": [
        "!tao model classification_tf1 evaluate -e $SPECS_DIR/classification_retrain_spec.cfg -k $KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re_b-_grhxvN"
      },
      "source": [
        "## 9. Visualize Inferences <a class=\"anchor\" id=\"head-9\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIjgWAg3hxvN"
      },
      "source": [
        "To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results. We'll run inference with the directory mode. You can also use the single image mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAxzcaxKhxvN"
      },
      "outputs": [],
      "source": [
        "# Defining the checkpoint epoch number to use for the subsequent steps.\n",
        "# This should be lesser than the number of epochs training has been run for, in case training was interrupted earlier.\n",
        "# By default, the default final model is at epoch 010.\n",
        "%env EPOCH=010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_99T8iThxvN"
      },
      "outputs": [],
      "source": [
        "!tao model classification_tf1 inference -e $SPECS_DIR/classification_retrain_spec.cfg \\\n",
        "                          -m $EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.hdf5 \\\n",
        "                          -k $KEY -b 32 -d $DATA_DIR/split/test/aeroplane \\\n",
        "                          -cm $EXPERIMENT_DIR/output_retrain/classmap.json \\\n",
        "                          --results_dir $EXPERIMENT_DIR/classification_inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dpFLJUfhxvO"
      },
      "source": [
        "As explained in Getting Started Guide, this outputs a results.csv file in the same directory. We can use a simple python program to see the visualize the output of csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P_Rx4ZehxvO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "from math import ceil\n",
        "\n",
        "DATA_DIR = os.environ.get('DATA_DIR')\n",
        "csv_path = os.path.join(os.getenv(\"EXPERIMENT_DIR\",\"/\"), 'classification_inference', 'result.csv')\n",
        "results = []\n",
        "with open(csv_path) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "        results.append((row[0], row[1]))\n",
        "\n",
        "w,h = 200,200\n",
        "fig = plt.figure(figsize=(30,30))\n",
        "columns = 5\n",
        "rows = 1\n",
        "for i in range(1, columns*rows + 1):\n",
        "    ax = fig.add_subplot(rows, columns,i)\n",
        "    print(results[i][0])\n",
        "    img = Image.open(results[i][0])\n",
        "    img = img.resize((w,h), Image.ANTIALIAS)\n",
        "    plt.imshow(img)\n",
        "    ax.set_title(results[i][1], fontsize=40)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "classification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "851240db530085c397391f2f949356c4eb3a8832b55aabc3de74eae9cba050e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}